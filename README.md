# QT5_ANN
#In this section, I have implemented a 4-layer (1 input layer with 15 neurons, 1 output layer with 26 neurons,
#and a bias node, 2 hidden layers with 100 neurons and a bias node in each layer; or described it as 15-100-100-26)
#deep feed forward neural networks from scratch using C++ in Qt Creator environment.
#At each hidden layer, one can could choose to use one of the Relu, Tanh, and Sigmoid activation functions.
#At the output layer, I have used the SoftMax units to generate a one-hot encoding for multiclass digit classification task.
#Each of the models was built on 3 different activation functions between layers were trained, the mean squared error and
#percentage of good classification were plotted to indicate the effectiveness of the activation functions.

#In general, the Relu activation function gained a better overall accuracy and a fast convergence for this task.

#The training dataset and test dataset is attached.
